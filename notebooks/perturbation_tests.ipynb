{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install \"transformers>=4.42\" \"datasets>=2.20\" \"scikit-learn>=1.5\""
      ],
      "metadata": {
        "id": "1aicyA2ey1pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    BertTokenizer,\n",
        "    BertForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "ciSZsSDdzHNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv(\"src/data/processed_bert/test_perturbation.csv\")"
      ],
      "metadata": {
        "id": "-l3Ie9Zi2W0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
        "    weighted_f1 = f1_score(labels, predictions, average='weighted')\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, predictions,\n",
        "        average=None,\n",
        "        labels=[0, 1, 2]\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1_macro': macro_f1,\n",
        "        'f1_weighted': weighted_f1,\n",
        "        'hate_speech_precision': precision[0],\n",
        "        'hate_speech_recall': recall[0],\n",
        "        'hate_speech_f1': f1[0],\n",
        "        'offensive_precision': precision[1],\n",
        "        'offensive_recall': recall[1],\n",
        "        'offensive_f1': f1[1],\n",
        "        'neither_precision': precision[2],\n",
        "        'neither_recall': recall[2],\n",
        "        'neither_f1': f1[2],\n",
        "    }"
      ],
      "metadata": {
        "id": "yERcU_fr11eN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_dataset(df, tokenizer, desc=\"original train\"):\n",
        "    dataset = Dataset.from_pandas(df[['clean_text', 'label']])\n",
        "\n",
        "    def tokenize_batch(examples):\n",
        "        return tokenizer(\n",
        "            examples['clean_text'],\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=128\n",
        "        )\n",
        "\n",
        "    tokenized = dataset.map(tokenize_batch, batched=True, desc=f\"Tokenizing {desc}\")\n",
        "    tokenized = tokenized.rename_column('label', 'labels')\n",
        "    tokenized = tokenized.remove_columns(['clean_text'])\n",
        "    tokenized.set_format('torch')\n",
        "\n",
        "    return tokenized"
      ],
      "metadata": {
        "id": "F0xb-R9d2FHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(model_name):\n",
        "  model_path = f\"/content/{model_name}/best_model\"\n",
        "  output_dir = f\"/content/{model_name}/output\"\n",
        "  model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "  tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "\n",
        "  args = TrainingArguments(\n",
        "      output_dir=output_dir,\n",
        "      per_device_eval_batch_size=32,\n",
        "      report_to='none',\n",
        "  )\n",
        "\n",
        "  trainer = Trainer(\n",
        "      model=model,\n",
        "      tokenizer=tokenizer,\n",
        "      compute_metrics=compute_metrics,\n",
        "      args=args\n",
        "  )\n",
        "\n",
        "  test_tok = tokenize_dataset(test_df, tokenizer, desc=\"test\")\n",
        "\n",
        "  print(\"Generating test predictions...\")\n",
        "  test_output = trainer.predict(test_tok)\n",
        "  test_preds = np.argmax(test_output.predictions, axis=1)\n",
        "  test_probs = torch.softmax(torch.tensor(test_output.predictions), dim=-1).numpy()\n",
        "\n",
        "  test_predictions_df = pd.DataFrame({\n",
        "        'true_label': test_df['label'].values,\n",
        "        'predicted_label': test_preds,\n",
        "        'prob_hate': test_probs[:, 0],\n",
        "        'prob_offensive': test_probs[:, 1],\n",
        "        'prob_neither': test_probs[:, 2]\n",
        "    })\n",
        "\n",
        "  test_predictions_df.to_csv(f\"{output_dir}/test_predictions.csv\", index=False)\n",
        "\n",
        "  metrics_df = pd.DataFrame([test_output.metrics])\n",
        "  metrics_df.to_csv(f\"{output_dir}/metrics.csv\", index=False)\n",
        "\n",
        "  print(f\"  Saved to {output_dir}/\")\n",
        "  return test_output.metrics\n"
      ],
      "metadata": {
        "id": "qR2oVHh5yy1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_orig = eval('bert_original')\n",
        "metrics_aug = eval('bert_augmented')"
      ],
      "metadata": {
        "id": "hnFHrSqMyTWC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}